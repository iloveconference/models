{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae18a72b",
   "metadata": {},
   "source": [
    "# Train a split classifier\n",
    "Given two texts, train a classifier to predict whether they belong to the same split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import cohere\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "from scipy.stats import loguniform, uniform, ttest_ind, mannwhitneyu\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from transformers import pipeline\n",
    "\n",
    "from models.split_model_train import get_labeled_pairs, \\\n",
    "    predict_using_syntactic_features, syntactic_paragraph_features,\\\n",
    "    predict_using_embeddings, predict_using_pairs, predict_using_features_and_embeddings, \\\n",
    "    predict_using_features_and_greedy_embeddings, predict_using_features_and_ensemble\n",
    "from models.split_utils import get_mpnet_embedder, get_openai_embedder, get_voyageai_embedder\n",
    "from models.split_model_eval import evaluate, compare, evaluate_embedder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b256625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure\n",
    "input_dir = '../data/split/labeled/'\n",
    "input_filename = '2023-09-23.json'\n",
    "\n",
    "output_dir = '../data/split/model/'\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "random_state = 42\n",
    "ngram_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e02209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'false'\n",
    "\n",
    "# spacy\n",
    "parser = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# cohere\n",
    "# cohere_api_key = os.environ['COHERE_KEY']\n",
    "# co = cohere.Client(cohere_api_key)\n",
    "# cohere_embedder = get_cohere_embedder(co)\n",
    "\n",
    "# bert-wiki-paragraphs\n",
    "# pipe = pipeline(\"text-classification\", model=\"dennlinger/bert-wiki-paragraphs\")\n",
    "# bert_wiki_paras_scorer = get_bert_wiki_paras_scorer(pipe)\n",
    "\n",
    "# mpnet\n",
    "mpnet = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "mpnet_embedder = get_mpnet_embedder(mpnet)\n",
    "\n",
    "# openai\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.Engine.list()\n",
    "openai_embedder = get_openai_embedder(openai)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd1e9f2-ca1a-41b8-8da9-1cf5b100f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())\n",
    "print(\"cuda total\", torch.cuda.get_device_properties(0).total_memory)\n",
    "print(\"cuda reserved\", torch.cuda.memory_reserved(0))\n",
    "print(\"cuda allocated\", torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541af210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read labeled data\n",
    "with open(os.path.join(input_dir, input_filename)) as f:\n",
    "    talk_sections = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a060ed",
   "metadata": {},
   "source": [
    "## Split the data into train and test sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a4271",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sections, test_sections = train_test_split(talk_sections, \n",
    "                                                 test_size=0.2, \n",
    "                                                 random_state=random_state)\n",
    "print('train', len(train_sections), 'test', len(test_sections))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e38f820",
   "metadata": {},
   "source": [
    "## First try various unsupervised approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28bd39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate grouping paragraphs by purely syntactic features, such as whether the paragraph is a list item or very short or a quote\n",
    "# the idea is to use purely syntactic features to group (a few) paragraphs that should be grouped,\n",
    "# but to never group paragraphs that shouldn't be grouped\n",
    "results = evaluate(train_sections, predict_using_syntactic_features(syntactic_paragraph_features), debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbeabff",
   "metadata": {},
   "source": [
    "### Let's try a few embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf6f48e-1609-4298-bc66-e5b07c126ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sections[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4aec91-d46d-4e6b-b960-0df6b087c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_embedder(paragraphs):\n",
    "    random_vectors = []\n",
    "    for _ in paragraphs:\n",
    "        # Generate a random vector with specified length\n",
    "        random_vector = np.random.rand(10)\n",
    "        random_vector /= np.linalg.norm(random_vector)\n",
    "        random_vectors.append(random_vector)\n",
    "    return random_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e9cf5f-276d-43df-9639-a915c47d40bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph random embeddings\n",
    "pos_sims, neg_sims = \\\n",
    "        evaluate_embedder(train_sections, results['predictions'], random_embedder)\n",
    "# larger ttest is better, smaller mann is better\n",
    "t_ttest, _ = ttest_ind(neg_sims, pos_sims, equal_var=False)\n",
    "t_mann, _  = mannwhitneyu(neg_sims, pos_sims, use_continuity=False)\n",
    "print(t_ttest, t_mann)\n",
    "plt.hist([pos_sims, neg_sims], np.linspace(0.0, 1.0, 100), label=['split', 'same'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc774b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph openai embeddings\n",
    "pos_sims, neg_sims = \\\n",
    "        evaluate_embedder(train_sections, results['predictions'], openai_embedder)\n",
    "# larger ttest is better, smaller mann is better\n",
    "t_ttest, _ = ttest_ind(neg_sims, pos_sims, equal_var=False)\n",
    "t_mann, _  = mannwhitneyu(neg_sims, pos_sims, use_continuity=False)\n",
    "print(t_ttest, t_mann)\n",
    "print('neg mean', sum(neg_sims)/len(neg_sims), 'pos mean', sum(pos_sims)/len(pos_sims))\n",
    "plt.hist([pos_sims, neg_sims], np.linspace(0.7, 1.0, 100), label=['split', 'same'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb33bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph mpnet embeddings\n",
    "pos_sims, neg_sims = \\\n",
    "        evaluate_embedder(train_sections, results['predictions'], mpnet_embedder)\n",
    "# larger ttest is better, smaller mann is better\n",
    "t_ttest, _ = ttest_ind(neg_sims, pos_sims, equal_var=False)\n",
    "t_mann, _  = mannwhitneyu(neg_sims, pos_sims, use_continuity=False)\n",
    "print(t_ttest, t_mann)\n",
    "print('neg mean', sum(neg_sims)/len(neg_sims), 'pos mean', sum(pos_sims)/len(pos_sims))\n",
    "plt.hist([pos_sims, neg_sims], np.linspace(0.0, 1.0, 100), label=['split', 'same'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a84e1-6c93-4170-9162-adc7c7346bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bge = SentenceTransformer('BAAI/bge-base-en-v1.5')\n",
    "\n",
    "def bge_embedder(paragraphs):\n",
    "    return bge.encode(paragraphs, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597f2ec-b0c1-454e-9e1c-a3e3427b3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph bge embeddings\n",
    "pos_sims, neg_sims = \\\n",
    "        evaluate_embedder(train_sections, results['predictions'], bge_embedder)\n",
    "# larger ttest is better, smaller mann is better\n",
    "t_ttest, _ = ttest_ind(neg_sims, pos_sims, equal_var=False)\n",
    "t_mann, _  = mannwhitneyu(neg_sims, pos_sims, use_continuity=False)\n",
    "print(t_ttest, t_mann)\n",
    "plt.hist([pos_sims, neg_sims], np.linspace(0.0, 1.0, 100), label=['split', 'same'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4241a9d-1979-4336-b2cb-96f5b22593b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "voyageai_embedder = get_voyageai_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c181f-e6fd-4286-8bb2-8e7f000b39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph voyageai embeddings\n",
    "pos_sims, neg_sims = \\\n",
    "        evaluate_embedder(train_sections, results['predictions'], voyageai_embedder)\n",
    "# larger ttest is better, smaller mann is better\n",
    "t_ttest, _ = ttest_ind(neg_sims, pos_sims, equal_var=False)\n",
    "t_mann, _  = mannwhitneyu(neg_sims, pos_sims, use_continuity=False)\n",
    "print(t_ttest, t_mann)\n",
    "plt.hist([pos_sims, neg_sims], np.linspace(0.7, 1.0, 100), label=['split', 'same'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97de717-7a30-4e98-81d2-c0cc6f88fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('neg mean', sum(neg_sims)/len(neg_sims), 'pos mean', sum(pos_sims)/len(pos_sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91405ac-c9e2-4b73-906b-0546562f5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([pos_sims, neg_sims], np.linspace(0.7, 1.0, 100), label=['split', 'same'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0dc797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a relatively long time and isn't as good as openai or mpnet embeddings\n",
    "\n",
    "# results = evaluate(train_sections, predict_using_pairs(score_bert_wiki_paras_scorer, 0.75),\n",
    "#                    debug=True)\n",
    "# results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b281592-24cb-4f21-8eaa-de9732480d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(train_sections, predict_using_embeddings(openai_embedder, 0.78),\n",
    "                   debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9317b-719c-436f-90e0-e3560225f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(train_sections, predict_using_embeddings(openai_embedder, 0.80),\n",
    "                   debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54991da2-d417-4536-bc6d-24021a36c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(train_sections, predict_using_embeddings(openai_embedder, 0.83),\n",
    "                   debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa24c9-e8d7-447a-8aea-d5512c667c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(train_sections, predict_using_embeddings(voyageai_embedder, 0.81),\n",
    "                   debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58cfff-d2b6-4858-a161-edd09e971bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(train_sections, predict_using_embeddings(voyageai_embedder, 0.82),\n",
    "                   debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d213dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(train_sections, predict_using_embeddings(voyageai_embedder, 0.83),\n",
    "                   debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55cdf0d-cf3a-4ce8-949e-4fca938889d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(train_sections, predict_using_embeddings(voyageai_embedder, 0.84),\n",
    "                   debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11800f13-5ff1-4a0a-bfb3-5d4d061d2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(train_sections, predict_using_embeddings(voyageai_embedder, 0.85),\n",
    "                   debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6e8fd-04f0-4083-9864-164430d9604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(train_sections, predict_using_embeddings(voyageai_embedder, 0.87),\n",
    "                   debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd6b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is very expensive and isn't any better than openai or mpnet embeddings\n",
    "\n",
    "# results = evaluate(train_sections, predict_using_embeddings(cohere_embedder, 3100.0),\n",
    "#                    debug=True)\n",
    "# results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(train_sections, predict_using_embeddings(mpnet_embedder, 0.425),\n",
    "                   debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c6750",
   "metadata": {},
   "source": [
    "### Try syntactic features followed by embeddings\n",
    "Use syntactic features to group some of the paragraphs,\n",
    "then use embeddings to segment the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(train_sections, predict_using_features_and_embeddings(syntactic_paragraph_features, voyageai_embedder, 0.83),\n",
    "                   debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a860a-d62c-419d-a95e-cb6e617d6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(train_sections, results['predictions'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fca33c-1858-463a-91aa-4659daffa23b",
   "metadata": {},
   "source": [
    "### Greedy with max characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9fbd1d-284b-4402-8442-f3a45c8edd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(train_sections, predict_using_features_and_greedy_embeddings(syntactic_paragraph_features, voyageai_embedder, 0.83, 2000),\n",
    "                   debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2b621-81ca-47f1-828d-0fec8d7974c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(train_sections, results['predictions'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2433f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(train_sections, predict_using_features_and_greedy_embeddings(syntactic_paragraph_features, openai_embedder, 0.80, 2000),\n",
    "                   debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(train_sections, results['predictions'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db665ef",
   "metadata": {},
   "source": [
    "### Run the two leading approaches on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b1bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(test_sections, predict_using_features_and_greedy_embeddings(syntactic_paragraph_features, voyageai_embedder, 0.83, 2000),\n",
    "                   debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae9b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(test_sections, predict_using_features_and_greedy_embeddings(syntactic_paragraph_features, openai_embedder, 0.80, 2000),\n",
    "                   debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be9fae-d062-4ab8-8a45-6a0675e67682",
   "metadata": {},
   "source": [
    "### Split paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d9db2-f428-4eb3-8f51-959dfc2a1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = predict_using_features_and_greedy_embeddings(syntactic_paragraph_features, voyageai_embedder, 0.83, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f41be59-f4cd-483a-b2e6-3a9294aefce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bba744-6dd5-4b6b-9e25-86a01c06a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = \"\\n\\n\".join([paragraph['text'] for paragraph in test_sections[0]['paragraphs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4897434-21d7-4bc5-859f-c0506f5effd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.split_utils import get_paragraph_texts_and_ids\n",
    "\n",
    "paragraph_texts_and_ids = get_paragraph_texts_and_ids(page_content)\n",
    "len(paragraph_texts_and_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c58fc2-538d-46b1-9758-8b0f71998bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, _id in paragraph_texts_and_ids:\n",
    "    print(_id, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96906aff-0590-4662-98a0-8464e612afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = [paragraph_text_id[0] for paragraph_text_id in paragraph_texts_and_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf5510-67f0-465f-9539-3f3afd5d410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = predict(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379489e9-2444-4408-ac6f-07bee5c1afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e41e71f-4110-4390-b7b2-76a37f513775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual\n",
    "print([paragraph['split'] for paragraph in test_sections[0]['paragraphs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd75fd4-6f7f-4814-ab5e-0b277dc82ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.split_utils import get_split_texts_and_ids\n",
    "\n",
    "split_texts_and_ids = get_split_texts_and_ids(\n",
    "    paragraph_texts_and_ids,\n",
    "    splits,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a4358a-0242-4498-8d3a-5a897b263a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, id in split_texts_and_ids:\n",
    "    print(id, text)\n",
    "    print('*****')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a73d1",
   "metadata": {},
   "source": [
    "## Train a custom segmentation classifier\n",
    "Use scores from the top segmentation approaches above\n",
    "as well as token and sentence counts to train an ensemble classifier\n",
    "\n",
    "**This is a lot of effort and isn't as good as using VoyageAI with a threshold of 0.83**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48db012",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = get_labeled_pairs(train_sections, openai_embedder, voyageai_embedder, parser, syntactic_paragraph_features)\n",
    "pair_df = pd.DataFrame(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d44cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c09374",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pair_df.drop(['label'], axis=1)\n",
    "y = pair_df['label']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d96d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune hyperparameters: LR or SVM\n",
    "\n",
    "# Logistic Regression\n",
    "clf = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=10000)\n",
    "params = {\n",
    "    'l1_ratio': uniform(0.0, 1.0),\n",
    "    'C': loguniform(1e-2, 1e3),\n",
    "}\n",
    "\n",
    "# SVM\n",
    "# clf = LinearSVC(dual=False, max_iter=10000)\n",
    "# params = {\n",
    "#     'C': loguniform(1e-6, 1e1),\n",
    "# }\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    clf,\n",
    "    params,\n",
    "    n_iter=100,\n",
    "    scoring='f1',\n",
    "    refit=False,\n",
    "    verbose=1,\n",
    "    n_jobs=8,\n",
    "    cv=10)\n",
    "search.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074bad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(search.best_params_)\n",
    "print(search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d52ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train clf over all training data\n",
    "\n",
    "# wrap SVM in calibrated classifier CV to get probabilities\n",
    "# svm = LinearSVC(dual=False, max_iter=10000, **search.best_params_)\n",
    "# clf = CalibratedClassifierCV(svm, cv=10)\n",
    "# clf.fit(X, y)\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(penalty='elasticnet', solver='saga', max_iter=10000, **search.best_params_)),\n",
    "])\n",
    "\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2243c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.feature_names_in_)\n",
    "print(clf['classifier'].coef_)\n",
    "print(clf['classifier'].intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df1f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate ensemble on training data\n",
    "results = evaluate(train_sections,\n",
    "                   predict_using_features_and_ensemble(syntactic_paragraph_features,\n",
    "                                                       openai_embedder,\n",
    "                                                       mpnet_embedder,\n",
    "                                                       parser,\n",
    "                                                       clf,\n",
    "                                                       0.55), debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e2ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate ensemble on test data\n",
    "results = evaluate(test_sections,\n",
    "                   predict_using_features_and_ensemble(syntactic_paragraph_features,\n",
    "                                                       openai_embedder,\n",
    "                                                       mpnet_embedder,\n",
    "                                                       parser,\n",
    "                                                       clf,\n",
    "                                                       0.55), debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1c66e",
   "metadata": {},
   "source": [
    "## Train over all data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dd85dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = get_labeled_pairs(talk_sections, openai_embedder, mpnet_embedder, parser, syntactic_paragraph_features)\n",
    "all_pair_df = pd.DataFrame(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_pair_df.drop(['label'], axis=1)\n",
    "y = all_pair_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd84b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.feature_names_in_)\n",
    "print(clf['classifier'].coef_)\n",
    "print(clf['classifier'].intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbc819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ensemble clf\n",
    "filename = os.path.join(output_dir, f\"{today}.pkl\")\n",
    "with open(filename,'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate ensemble on all data\n",
    "results = evaluate(talk_sections,\n",
    "                   predict_using_features_and_ensemble(syntactic_paragraph_features,\n",
    "                                                       openai_embedder,\n",
    "                                                       mpnet_embedder,\n",
    "                                                       parser,\n",
    "                                                       clf,\n",
    "                                                       0.55), debug=True)\n",
    "results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fce8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilc-models",
   "language": "python",
   "name": "ilc-models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
