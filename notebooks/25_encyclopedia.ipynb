{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed2304b",
   "metadata": {},
   "source": [
    "# Crawl, load, and split Enclopedia of Mormonism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe5bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Iterator\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.crawl_utils import get_page, save_page\n",
    "from models.load_encyclopedia import load_encyclopedia\n",
    "from models.load_utils import Loader, load_docs_from_jsonl, save_docs_to_jsonl\n",
    "from models.split_model import MarkdownSyntacticEmbeddingSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ece895",
   "metadata": {},
   "source": [
    "## Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "source = \"encyclopedia\"\n",
    "base = 'https://eom.byu.edu/'\n",
    "host = ' https://eom.byu.edu/index.php?title=Special:AllPages'\n",
    "crawl_dir = f'../data/raw/{source}'\n",
    "bs_parser = 'html.parser'\n",
    "delay_seconds = 15\n",
    "\n",
    "if not os.path.exists(crawl_dir):\n",
    "    os.makedirs(crawl_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(url):\n",
    "    query_components = urlparse(url).query.split('=')\n",
    "    path = query_components[-1].lower()\n",
    "    path = re.sub(r'\\W+', '-', path)\n",
    "    return os.path.join(crawl_dir, f\"{path}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9225f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hrefs_from_div(soup, base):\n",
    "    hrefs = []\n",
    "\n",
    "    # Find the div with class \"mw-allpages-body\"\n",
    "    div_with_class = soup.find('div', class_='mw-allpages-body')\n",
    "\n",
    "    if div_with_class:\n",
    "        # Find all <a> tags within the div\n",
    "        a_tags = div_with_class.find_all('a')\n",
    "\n",
    "        for a_tag in a_tags:\n",
    "            if \"class\" in a_tag.attrs and \"mw-redirect\" in a_tag[\"class\"]:\n",
    "                continue\n",
    "                \n",
    "            # Get the href attribute\n",
    "            href = a_tag.get('href')\n",
    "\n",
    "            if href:\n",
    "                # Make the href an absolute URL based on the base_url\n",
    "                absolute_url = urljoin(base, href)\n",
    "                hrefs.append(absolute_url)\n",
    "\n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8861659b-9805-45a3-a931-cfb74a293949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_next_page_href(soup, base):\n",
    "    # Find the div with class \"mw-allpages-nav\"\n",
    "    div_with_class = soup.find('div', class_='mw-allpages-nav')\n",
    "\n",
    "    if div_with_class:\n",
    "        # Find all <a> tags within the div\n",
    "        a_tags = div_with_class.find_all('a')\n",
    "\n",
    "        for a_tag in a_tags:\n",
    "            # Check if the text of the <a> tag starts with \"Next page\"\n",
    "            if a_tag.text.startswith(\"Next page\"):\n",
    "                # Get the href attribute\n",
    "                href = a_tag.get('href')\n",
    "                if href:\n",
    "                    # Make the href an absolute URL based on the base_url\n",
    "                    absolute_url = urljoin(base, href)\n",
    "                    return absolute_url\n",
    "\n",
    "    # If no matching <a> tag is found, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e1d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting URL\n",
    "start_url = host\n",
    "\n",
    "# Initialize an empty list to store all hrefs\n",
    "all_hrefs = []\n",
    "\n",
    "while start_url:\n",
    "    # Fetch the content of the current page\n",
    "    print('fetch', start_url)\n",
    "    status_code, html = get_page(start_url, delay_seconds)\n",
    "    if status_code != 200:\n",
    "        print(f\"Failed to fetch {start_url}\")\n",
    "        break\n",
    "\n",
    "    # Create a BeautifulSoup object from the HTML content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Extract hrefs and the URL of the next page\n",
    "    hrefs = extract_hrefs_from_div(soup, base)\n",
    "    next_page_url = extract_next_page_href(soup, base)\n",
    "\n",
    "    # Add the extracted hrefs to the list\n",
    "    print('found', len(hrefs))\n",
    "    all_hrefs.extend(hrefs)\n",
    "\n",
    "    # If there is a next page URL, update the start_url for the next iteration\n",
    "    if next_page_url:\n",
    "        start_url = next_page_url\n",
    "    else:\n",
    "        # If there is no next page URL, break the loop\n",
    "        break\n",
    "\n",
    "len(all_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa244c64-5d40-4d93-ac95-e5fce805b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for href in all_hrefs:\n",
    "    path_file =  get_path(href)\n",
    "    print(href, path_file)\n",
    "    if os.path.exists(path_file):\n",
    "        continue\n",
    "    status_code, html = get_page(href, delay_seconds)\n",
    "    if status_code != 200:\n",
    "        print(\"Error!\", status_code , href)\n",
    "        continue\n",
    "    save_page(path_file,href,html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af220c",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "# input_dir is now crawl_dir, and output_dir is now load_dir, and output_filename is now load_filename\n",
    "load_dir = f'../data/load/{source}/'\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "load_filename = os.path.join(load_dir, f\"{today}.jsonl\")\n",
    "\n",
    "if not os.path.exists(load_dir):\n",
    "    os.makedirs(load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc10596",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(load_encyclopedia, crawl_dir)\n",
    "docs = loader.load(verbose=True)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8755943",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"metadat: \", docs[0].metadata)\n",
    "print()\n",
    "print(\"content: \", docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c3f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_docs_to_jsonl(docs, load_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4ec07",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d47019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure\n",
    "# input_path is now load_filename, output_dir is now split_dir, and output filename is now split_filename\n",
    "split_dir = f'../data/split/{source}/'\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "# output filename is now split_filename\n",
    "split_filename = os.path.join(split_dir, f\"{today}.jsonl\")\n",
    "\n",
    "if not os.path.exists(split_dir):\n",
    "    os.makedirs(split_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ff1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_docs_from_jsonl(load_filename)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = MarkdownSyntacticEmbeddingSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs, verbose=True)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, split in enumerate(splits[:25]):\n",
    "    print(ix, split.metadata)\n",
    "    print(split.page_content)\n",
    "    print(\"\\n!!! SPLIT !!!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_docs_to_jsonl(splits, split_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c49eaf4-677e-439c-8d9e-0e494b1d3c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scqa-models",
   "language": "python",
   "name": "scqa-models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
