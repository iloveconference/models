{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute NDCG Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from models.rank_eval import get_ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write functions to read two different csv files - scored-results.csv and question-results.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the first CSV file with \"Question,\" \"ResultId,\" and \"Score\" columns\n",
    "def read_true_csv(file_path):\n",
    "    data_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            question = row.get('Question', '')  # Use get to handle missing 'Question' key\n",
    "            result_id = row.get('ResultId', '')  # Use get to handle missing 'ResultId' key\n",
    "            score = float(row.get('Score', 0))  # Use get to handle missing 'Score' key and convert to float\n",
    "            if question and result_id:  # Check if both 'Question' and 'ResultId' are present\n",
    "                if question not in data_dict:\n",
    "                    data_dict[question] = []\n",
    "                data_dict[question].append({'id': result_id, 'score': score})\n",
    "    return data_dict\n",
    "\n",
    "# Function to read the second CSV file with \"Question,\" \"ResultId,\" and \"ResultRank\" columns\n",
    "def read_predicted_csv(file_path):\n",
    "    data_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            question = row.get('Question', '')  # Use get to handle missing 'Question' key\n",
    "            result_id = row.get('ResultId', '')  # Use get to handle missing 'ResultId' key\n",
    "            result_rank = int(row.get('ResultRank', 0))  # Use get to handle missing 'ResultRank' key and convert to int\n",
    "            if question and result_id:  # Check if both 'Question' and 'ResultId' are present\n",
    "                if question not in data_dict:\n",
    "                    data_dict[question] = []\n",
    "                data_dict[question].append({'id': result_id, 'score': result_rank})\n",
    "    return data_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function to compute the NDCG scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate NDCG scores\n",
    "def calculate_ndcg(true_results, predicted_results, k_values):\n",
    "    ndcg_scores = {}\n",
    "    single_document_questions = []  # Keep track of questions with only one document\n",
    "    \n",
    "    for question in true_results:\n",
    "        true_list = true_results[question]\n",
    "        predicted_list = predicted_results.get(question, [])\n",
    "        \n",
    "        if len(true_list) == 1:\n",
    "            # Append details of questions with only one document\n",
    "            single_document_questions.append({\n",
    "                'question': question,\n",
    "                'true_result': true_list[0],\n",
    "                'predicted_result': predicted_list[0] if predicted_list else None\n",
    "            })\n",
    "        else:\n",
    "            # Calculate NDCG scores for questions with more than one document\n",
    "            ndcg_scores[question] = [get_ndcg(true_list, predicted_list, k) for k in k_values]\n",
    "    \n",
    "    # Print questions with only one document\n",
    "    print(\"Questions with Only One Document:\")\n",
    "    for item in single_document_questions:\n",
    "        print(f\"Question: {item['question']}\")\n",
    "        print(f\"True Result: {item['true_result']}\")\n",
    "        print(f\"Predicted Result: {item['predicted_result']}\")\n",
    "        print()\n",
    "    \n",
    "    return ndcg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the functions to read the csv files and to compute NDCG scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "true_results_file = '../references/scored-results.csv'\n",
    "predicted_results_file = '../data/exports/question-results.csv'\n",
    "    \n",
    "# Read the first CSV file with true results\n",
    "true_results = read_true_csv(true_results_file)\n",
    "\n",
    "# Read the second CSV file with predicted results\n",
    "predicted_results = read_predicted_csv(predicted_results_file)\n",
    "\n",
    "# Calculate NDCG scores\n",
    "k_values = [5, 3, 10]\n",
    "ndcg_scores = calculate_ndcg(true_results, predicted_results, k_values)\n",
    "# print(\"predicted results\", predicted_results)\n",
    "# print()\n",
    "# print(\"true results\", true_results)\n",
    "# Print NDCG scores\n",
    "print(\"NDCG Scores:\")\n",
    "for question, scores in ndcg_scores.items():\n",
    "    print(f\"Question: {question}, NDCG Scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ndcg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print average NDCG scores\n",
    "avg_ndcg_scores = {k: sum(scores) / len(scores) for k, scores in zip(k_values, zip(*ndcg_scores.values()))}\n",
    "print(\"\\nAverage NDCG Scores:\")\n",
    "for k, avg_score in avg_ndcg_scores.items():\n",
    "    print(f\"At k={k}: {avg_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "models-5vTMPdpX-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
