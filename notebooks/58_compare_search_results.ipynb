{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute NDCG Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.rank_eval import get_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "true_results_file = '../references/scored-results-45.csv'\n",
    "predicted_results_file = '../data/exports/question-results-hyde-openai.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write functions to read two different csv files - scored-results.csv and question-results.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the first CSV file with \"Question,\" \"ResultId,\" and \"Score\" columns\n",
    "def read_true_csv(file_path):\n",
    "    data_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            question = row.get('Question', '')  # Use get to handle missing 'Question' key\n",
    "            result_id = row.get('ResultId', '')  # Use get to handle missing 'ResultId' key\n",
    "            score = float(row.get('Score', 0))  # Use get to handle missing 'Score' key and convert to float\n",
    "            if question and result_id:  # Check if both 'Question' and 'ResultId' are present\n",
    "                if question not in data_dict:\n",
    "                    data_dict[question] = []\n",
    "                data_dict[question].append({'id': result_id, 'score': score})\n",
    "    return data_dict\n",
    "\n",
    "# Function to read the second CSV file with \"Question,\" \"ResultId,\" and \"ResultRank\" columns\n",
    "def read_predicted_csv(file_path):\n",
    "    data_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            question = row.get('Question', '')  # Use get to handle missing 'Question' key\n",
    "            result_id = row.get('ResultId', '')  # Use get to handle missing 'ResultId' key\n",
    "            result_rank = int(row.get('ResultRank', 0))  # Use get to handle missing 'ResultRank' key and convert to int\n",
    "            result_similarity = float(row.get('ResultScore', 0.0))  # Use get to handle missing 'ResultScore' key and convert to float\n",
    "            if question and result_id:  # Check if both 'Question' and 'ResultId' are present\n",
    "                if question not in data_dict:\n",
    "                    data_dict[question] = []\n",
    "                data_dict[question].append({'id': result_id, 'score': result_rank, 'similarity': result_similarity})\n",
    "    return data_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function to compute the NDCG scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert ranks 1..20 to scores 20..1\n",
    "# get_ndcg needs the top-ranked item (rank=1) to have the highest score (20) \n",
    "# and the bottom-ranked item (rank=20) to have the lowest score(1)\n",
    "def ranks_to_scores(ranks):\n",
    "    return [{'id': rank['id'], 'score': len(ranks)-rank['score']} for rank in ranks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate NDCG scores\n",
    "def calculate_ndcg(true_results, predicted_results, k_values):\n",
    "    ndcg_scores = {}\n",
    "    single_document_questions = []  # Keep track of questions with only one document\n",
    "    \n",
    "    for question in true_results:\n",
    "        true_list = true_results[question]\n",
    "        predicted_list = predicted_results.get(question, [])\n",
    "\n",
    "        if len(true_list) == 1:\n",
    "            # Append details of questions with only one document\n",
    "            single_document_questions.append({\n",
    "                'question': question,\n",
    "                'true_result': true_list[0],\n",
    "                'predicted_result': predicted_list[0] if predicted_list else None\n",
    "            })\n",
    "        else:\n",
    "            # Calculate NDCG scores for questions with more than one document\n",
    "            ndcg_scores[question] = [get_ndcg(true_list, ranks_to_scores(predicted_list), k) for k in k_values]\n",
    "    \n",
    "    # Print questions with only one document\n",
    "    if len(single_document_questions) > 0:\n",
    "        print(\"Questions with Only One Document:\")\n",
    "    for item in single_document_questions:\n",
    "        print(f\"Question: {item['question']}\")\n",
    "        print(f\"True Result: {item['true_result']}\")\n",
    "        print(f\"Predicted Result: {item['predicted_result']}\")\n",
    "        print()\n",
    "    \n",
    "    return ndcg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the functions to read the csv files and to compute NDCG scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first CSV file with true results\n",
    "true_results = read_true_csv(true_results_file)\n",
    "\n",
    "# Read the second CSV file with predicted results\n",
    "predicted_results = read_predicted_csv(predicted_results_file)\n",
    "\n",
    "# Calculate NDCG scores\n",
    "k_values = [5, 3, 10]\n",
    "ndcg_scores = calculate_ndcg(true_results, predicted_results, k_values)\n",
    "# print(\"predicted results\", predicted_results)\n",
    "# print()\n",
    "# print(\"true results\", true_results)\n",
    "# Print NDCG scores\n",
    "print(\"NDCG Scores:\")\n",
    "for question, scores in ndcg_scores.items():\n",
    "    print(f\"Question: {question}, NDCG Scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ndcg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average NDCG scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print average NDCG scores\n",
    "avg_ndcg_scores = {k: sum(scores) / len(scores) for k, scores in zip(k_values, zip(*ndcg_scores.values()))}\n",
    "print(\"\\nAverage NDCG Scores:\")\n",
    "for k, avg_score in avg_ndcg_scores.items():\n",
    "    print(f\"At k={k}: {avg_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at an individual question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Why do you serve missions?\"\n",
    "k = 5\n",
    "\n",
    "ndcg = get_ndcg(true_results[q], ranks_to_scores(predicted_results[q]), k)\n",
    "print(f\"NDCG={ndcg}\")\n",
    "\n",
    "print(f\"Scores of the top {k} search results\")\n",
    "scores = {id_score['id']: id_score['score'] for id_score in true_results[q]}\n",
    "rankings = {id_score['score']: id_score['id'] for id_score in predicted_results[q]}\n",
    "for ix in range(1, k+1):\n",
    "    _id = rankings.get(ix, 0)\n",
    "    print(ix, scores.get(_id, 0), _id)\n",
    "\n",
    "print(\"Ideal top search results\")\n",
    "sorted_scores = sorted(true_results[q], key=lambda x: x['score'], reverse=True)\n",
    "curr_score = 3\n",
    "for ix in range(len(sorted_scores)):\n",
    "    id_score = sorted_scores[ix]\n",
    "    if id_score['score'] < curr_score:\n",
    "        if ix >= k:\n",
    "            break\n",
    "        print('-----')\n",
    "    print(ix+1, id_score['score'], id_score['id'])\n",
    "    curr_score = id_score['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph scores of relevant vs irrelevant results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_3 = []\n",
    "scores_2 = []\n",
    "scores_1 = []\n",
    "scores_0 = []\n",
    "not_found_results = 0\n",
    "for question in ndcg_scores:\n",
    "    for predicted_result in predicted_results[question]:\n",
    "        found = False\n",
    "        score = None\n",
    "        similarity = None\n",
    "        for true_result in true_results[question]:\n",
    "            if true_result['id'] == predicted_result['id']:\n",
    "                score = true_result['score']\n",
    "                similarity = predicted_result['similarity']\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            not_found_results += 1\n",
    "            print('result not found', predicted_result['id'], question)\n",
    "        elif score == 3.0:\n",
    "            scores_3.append(similarity)\n",
    "        elif score == 2.0:\n",
    "            scores_2.append(similarity)\n",
    "        elif score == 1.0:\n",
    "            scores_1.append(similarity)\n",
    "        elif score == 0.0:\n",
    "            scores_0.append(similarity)\n",
    "        else:\n",
    "            print('unexpected score', score, predicted_result['id'], question)\n",
    "print(not_found_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(scores_3, bins=20, alpha=0.25, label=\"3\", color='blue')\n",
    "plt.hist(scores_2, bins=20, alpha=0.25, label=\"2\", color='green')\n",
    "plt.hist(scores_1, bins=20, alpha=0.25, label=\"1\", color='yellow')\n",
    "plt.hist(scores_0, bins=20, alpha=0.25, label=\"0\", color='red')\n",
    "plt.title('Overlapping Histogram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
