{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936d02dd",
   "metadata": {},
   "source": [
    "# Crawl Encyclopedia talks from the Church of Jesus Christ of Latter-day Saints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe5bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from models.crawl_utils import get_page, save_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "base = 'https://eom.byu.edu/'\n",
    "host = ' https://eom.byu.edu/index.php?title=Special:AllPages'\n",
    "base_dir = '../data/raw/encyclopedia'\n",
    "bs_parser = 'html.parser'\n",
    "delay_seconds = 5\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d2e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hrefs_from_mw_allpages_body(html):\n",
    "    hrefs = []\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find all div tags with the class 'views-field-title'\n",
    "    divs_with_class = soup.find_all('div', class_='mw-allpages-body')\n",
    "\n",
    "    for div in divs_with_class:\n",
    "        # Find all anchor tags within the div\n",
    "        anchor_tags = div.find_all('a')\n",
    "\n",
    "        # Extract href attribute from each anchor tag and append to the list\n",
    "        for anchor in anchor_tags:\n",
    "            href = anchor.get('href')\n",
    "            if href:\n",
    "                hrefs.append(urljoin(host, href))\n",
    "\n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8861659b-9805-45a3-a931-cfb74a293949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(url):\n",
    "    path_components = urlparse(url).path.split('/')\n",
    "    return os.path.join(base_dir, f\"{path_components[-1]}.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f5f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(soup, base):\n",
    "    hrefs = []\n",
    "\n",
    "    div = soup.find('div', class_ ='mw-allpages-body')\n",
    "    if not div:\n",
    "        return hrefs\n",
    "\n",
    "    # Find all div tags with the class 'views-field-title'\n",
    "    link_tags = soup.find_all('div', {'mw_allpages_body': True})\n",
    "\n",
    "    # Extract href attribute from each anchor tag and append to the list\n",
    "    for link in link_tags:\n",
    "        href = link.get('mw-allpages-body')\n",
    "        if href:\n",
    "            if href.startswith(base):\n",
    "                hrefs.append(href)\n",
    "                # path_components = urlparse(href).path.split('/')\n",
    "                # print(path_components)\n",
    "                # if len(path_components) == 4:\n",
    "                #    if not href in hrefs:\n",
    "                #        hrefs.append(href)\n",
    "\n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f090d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hrefs_from_div(soup, base):\n",
    "    hrefs = []\n",
    "\n",
    "    # Find the div with class \"mw-allpages-body\"\n",
    "    div_with_class = soup.find('div', class_='mw-allpages-body')\n",
    "\n",
    "    if div_with_class:\n",
    "        # Find all <a> tags within the div\n",
    "        a_tags = div_with_class.find_all('a')\n",
    "\n",
    "        for a_tag in a_tags:\n",
    "            # Get the href attribute\n",
    "            href = a_tag.get('href')\n",
    "\n",
    "            if href:\n",
    "                # Make the href an absolute URL based on the base_url\n",
    "                absolute_url = urljoin(base, href)\n",
    "                hrefs.append(absolute_url)\n",
    "\n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_next_page_href(soup, base):\n",
    "    # Find the div with class \"mw-allpages-nav\"\n",
    "    div_with_class = soup.find('div', class_='mw-allpages-nav')\n",
    "\n",
    "    if div_with_class:\n",
    "        # Find all <a> tags within the div\n",
    "        a_tags = div_with_class.find_all('a')\n",
    "\n",
    "        for a_tag in a_tags:\n",
    "            # Check if the text of the <a> tag starts with \"Next page\"\n",
    "            if a_tag.text.startswith(\"Next page\"):\n",
    "                # Get the href attribute\n",
    "                href = a_tag.get('href')\n",
    "                if href:\n",
    "                    # Make the href an absolute URL based on the base_url\n",
    "                    absolute_url = urljoin(base, href)\n",
    "                    return absolute_url\n",
    "\n",
    "    # If no matching <a> tag is found, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d33629-31c0-4968-8346-db60b0549e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Starting URL\n",
    "start_url = 'https://eom.byu.edu/index.php?title=Special:AllPages'\n",
    "\n",
    "# Initialize an empty list to store all hrefs\n",
    "all_hrefs = []\n",
    "\n",
    "while start_url:\n",
    "    # Fetch the content of the current page\n",
    "    status_code, html = get_page(start_url)\n",
    "    if status_code != 200:\n",
    "        print(f\"Failed to fetch {start_url}\")\n",
    "        break\n",
    "\n",
    "    # Create a BeautifulSoup object from the HTML content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Extract hrefs and the URL of the next page\n",
    "    hrefs = extract_hrefs_from_div(soup, base)\n",
    "    next_page_url = extract_next_page_href(soup, base)\n",
    "\n",
    "    # Add the extracted hrefs to the list\n",
    "    all_hrefs.extend(hrefs)\n",
    "\n",
    "    # If there is a next page URL, update the start_url for the next iteration\n",
    "    if next_page_url:\n",
    "        start_url = next_page_url\n",
    "    else:\n",
    "        # If there is no next page URL, break the loop\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d61cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for href in all_hrefs:\n",
    "    print(href)\n",
    "    path_file =  get_path(href)\n",
    "    if os.path.exists(path_file):\n",
    "        continue\n",
    "    status_code, html = get_page(href, delay_seconds)\n",
    "    if status_code != 200:\n",
    "        print(\"Error!\", status_code , href)\n",
    "        continue\n",
    "    save_page(path_file,href,html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f4bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "models",
   "language": "python",
   "name": "models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
