{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936d02dd",
   "metadata": {},
   "source": [
    "# Crawl Pearl of Great Price from the Church of Jesus Christ of Latter-day Saints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe5bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from models.crawl_utils import get_page, save_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "hosts = ['https://pearlofgreatpricecentral.org/category/book-of-abraham/', 'https://pearlofgreatpricecentral.org/category/joseph-smith-history/']\n",
    "base_dir = '../data/raw/pearl_of_great_price'\n",
    "bs_parser = 'html.parser'\n",
    "delay_seconds = 5\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8074aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hrefs_from_elementor_div(soup, base_url):\n",
    "    hrefs = []    \n",
    "    # Find the div with class 'elementor-posts-container'\n",
    "    elementor_div = soup.find('div', class_='elementor-posts-container')\n",
    "    # Check if the div is found\n",
    "    if elementor_div:\n",
    "        # Find all 'a' tags within the div\n",
    "        a_tags = elementor_div.find_all('a', href=True)\n",
    "        # Extract and normalize href values\n",
    "        for a_tag in a_tags:\n",
    "            href = a_tag['href']\n",
    "            full_url = urljoin(base_url, href)\n",
    "            if not full_url in hrefs:\n",
    "                hrefs.append(full_url)    \n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_next_sibling_href(soup, base_url):\n",
    "    # Find the span tag with classes 'page-numbers' and 'current'\n",
    "    span_tag = soup.find('span', class_='page-numbers current')    \n",
    "    if span_tag:\n",
    "        # Find the next sibling anchor tag\n",
    "        anchor_tag = span_tag.find_next_sibling('a', href=True)\n",
    "        if anchor_tag:\n",
    "            href = anchor_tag['href']\n",
    "            full_url = urljoin(base_url, href)\n",
    "            return full_url\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b1de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_extract_hrefs(start_url, base_url, max_pages=10):\n",
    "    # List to store all extracted hrefs\n",
    "    all_hrefs = []\n",
    "    # Loop to fetch pages and extract hrefs\n",
    "    for _ in range(max_pages):\n",
    "        # Fetch the current page\n",
    "        status_code, html = get_page(start_url)\n",
    "        if status_code == 200:\n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            # Extract hrefs from the current page\n",
    "            hrefs = extract_hrefs_from_elementor_div(soup, base_url)\n",
    "            # Add the extracted hrefs to the list\n",
    "            all_hrefs.extend(hrefs)\n",
    "            # Get the href for the next page\n",
    "            next_page_href = extract_next_sibling_href(soup, base_url)\n",
    "            if next_page_href:\n",
    "                # Update the start_url for the next iteration\n",
    "                start_url = next_page_href\n",
    "            else:\n",
    "                print(\"No next page found. Exiting loop.\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Failed to fetch page: {start_url}\")\n",
    "            break\n",
    "    return all_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hrefs = []\n",
    "for start_url in hosts:\n",
    "    hrefs = fetch_and_extract_hrefs(start_url, start_url)\n",
    "    all_hrefs.extend(hrefs)\n",
    "print(all_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8861659b-9805-45a3-a931-cfb74a293949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(url):\n",
    "    path_components = urlparse(url).path.split('/')\n",
    "    return os.path.join(base_dir, f\"{path_components[-2]}.json\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa244c64-5d40-4d93-ac95-e5fce805b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in all_hrefs:\n",
    "    path_file =  get_path(url)\n",
    "    print(path_file)\n",
    "    if os.path.exists(path_file):\n",
    "        continue\n",
    "    status_code, html = get_page(url, delay_seconds)\n",
    "    if status_code != 200:\n",
    "        print(\"Error!\", status_code , url)\n",
    "        continue\n",
    "    save_page(path_file,url,html)    \n",
    "print(\"End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "models-9zvO7eJO-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
